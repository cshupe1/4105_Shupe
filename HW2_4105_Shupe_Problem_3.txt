import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load the housing datasheet
file_path = '/content/drive/MyDrive/Housing.csv'
data = pd.read_csv(file_path)

# Define the input features and target variable for 3.a and 3.b
X = data[['area', 'bedrooms', 'bathrooms', 'stories', 'parking']]
y = data['price']

# Separate the training and validation sets
split_ratio = 0.8
num_samples = len(data)
num_train_samples = int(split_ratio * num_samples)
X_train, X_val = X[:num_train_samples], X[num_train_samples:]
y_train, y_val = y[:num_train_samples], y[num_train_samples:]

# Define preprocessing functions
def normalize(X):
    return (X - X.mean()) / X.std()

def standardize(X):
    return (X - X.mean()) / X.std()

# Preprocess the input features for 3.a and 3.b
X_train_normalized = normalize(X_train)
X_val_normalized = X_val  # No normalization for evaluation set

X_train_standardized = standardize(X_train)
X_val_standardized = X_val  # No standardization for evaluation set

# Add a column of ones for bias term for both cases
X_train_normalized.insert(0, 'ones', 1)
X_val_normalized.insert(0, 'ones', 1)
X_train_standardized.insert(0, 'ones', 1)

# Convert to NumPy arrays for both cases
X_train_normalized = X_train_normalized.to_numpy()
X_val_normalized = X_val_normalized.to_numpy()
X_train_standardized = X_train_standardized.to_numpy()
X_val_standardized = X_val_standardized.to_numpy()
y_train = y_train.to_numpy()
y_val = y_val.to_numpy()

# Function for gradient descent with L2 regularization
def gradient_descent_with_regularization(X, y, theta, alpha, iterations, lambda_reg):
    m = len(y)
    history = []

    for _ in range(iterations):
        error = X.dot(theta) - y
        gradient = (X.T.dot(error) + lambda_reg * theta) / m
        theta = theta - (alpha * gradient).astype('float64')
        cost = (np.sum(error ** 2) + lambda_reg * np.sum(theta[1:] ** 2)) / (2 * m)  # L2 regularization term
        history.append(cost)

    return theta, history

# Hyperparameters
alpha_values = [0.1, 0.05, 0.01]
iterations = 1000
lambda_reg = 0.01  # Regularization parameter

# Initialize parameters (thetas) to zeros with the correct dimension for 3.a and 3.b
initial_theta = np.zeros(X_train_normalized.shape[1])

# Dictionaries to store losses for 3.a and 3.b
losses_normalized_3a = {}
losses_standardized_3a = {}
losses_normalized_3b = {}
losses_standardized_3b = {}

# Perform gradient descent with regularization and record losses for input normalization and input standardization for 3.a and 3.b
for alpha in alpha_values:
    # 3.a: Input normalization
    theta_normalized_3a, history_normalized_3a = gradient_descent_with_regularization(X_train_normalized, y_train, initial_theta, alpha, iterations, lambda_reg)
    losses_normalized_3a[alpha] = history_normalized_3a

    # 3.a: Input standardization
    theta_standardized_3a, history_standardized_3a = gradient_descent_with_regularization(X_train_standardized, y_train, initial_theta, alpha, iterations, lambda_reg)
    losses_standardized_3a[alpha] = history_standardized_3a

    # 3.b: Input normalization
    theta_normalized_3b, history_normalized_3b = gradient_descent_with_regularization(X_train_normalized, y_train, initial_theta, alpha, iterations, lambda_reg)
    losses_normalized_3b[alpha] = history_normalized_3b

    # 3.b: Input standardization
    theta_standardized_3b, history_standardized_3b = gradient_descent_with_regularization(X_train_standardized, y_train, initial_theta, alpha, iterations, lambda_reg)
    losses_standardized_3b[alpha] = history_standardized_3b

# Plot the training and evaluation losses for both input scaling approaches for 3.a and 3.b
plt.figure(figsize=(16, 8))

# Plot the training and evaluation losses for input normalization for 3.a
plt.subplot(2, 2, 1)
for alpha, loss_history in losses_normalized_3a.items():
    plt.plot(range(iterations), loss_history, label=f'3.a Normalization, Alpha={alpha}')
plt.title('Training Loss vs. Iterations (3.a Normalization)')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.legend()

# Plot the training and evaluation losses for input standardization for 3.a
plt.subplot(2, 2, 2)
for alpha, loss_history in losses_standardized_3a.items():
    plt.plot(range(iterations), loss_history, linestyle='--', label=f'3.a Standardization, Alpha={alpha}')
plt.title('Training Loss vs. Iterations (3.a Standardization)')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.legend()

# Plot the training and evaluation losses for input normalization for 3.b
plt.subplot(2, 2, 3)
for alpha, loss_history in losses_normalized_3b.items():
    plt.plot(range(iterations), loss_history, label=f'3.b Normalization, Alpha={alpha}')
plt.title('Training Loss vs. Iterations (3.b Normalization)')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.legend()

# Plot the training and evaluation losses for input standardization for 3.b
plt.subplot(2, 2, 4)
for alpha, loss_history in losses_standardized_3b.items():
    plt.plot(range(iterations), loss_history, linestyle='--', label=f'3.b Standardization, Alpha={alpha}')
plt.title('Training Loss vs. Iterations (3.b Standardization)')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()
